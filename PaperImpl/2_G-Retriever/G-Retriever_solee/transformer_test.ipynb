{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solee/miniconda3/envs/g_retriever/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "pretrained_repo = 'sentence-transformers/all-roberta-large-v1'\n",
    "batch_size = 256  # Adjust the batch size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   100,   524,    10,  3645,    13,    61,    38,    74,   101,\n",
      "             7,   120,    63, 33183, 11303,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solee/miniconda3/envs/g_retriever/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_repo)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_texts = [\n",
    "    'I am a sentence for which I would like to get its embedding'\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "\n",
    "encoding = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids=None, attention_mask=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"att_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"input_ids\"].size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = index.item()\n",
    "        batch_data = dict()\n",
    "        for key in self.data.keys():\n",
    "            if self.data[key] is not None:\n",
    "                batch_data[key] = self.data[key][index]\n",
    "        return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: <__main__.Dataset object at 0x7fb048e8e1c0>\n",
      "DataLoader: <torch.utils.data.dataloader.DataLoader object at 0x7fb048e8e670>\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "print(\"Dataset:\", dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"DataLoader:\", dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_Bert(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_repo):\n",
    "        super(Sentence_Bert, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_repo)\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        data_type = token_embeddings.dtype\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).to(data_type)\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def forward(self, input_ids, att_mask):\n",
    "        bert_out = self.bert_model(input_ids=input_ids, attention_mask=att_mask)\n",
    "        print(\"input_ids: \", input_ids)\n",
    "        print(\"bert_out: \", bert_out)\n",
    "        sentence_embeddings = self.mean_pooling(bert_out, att_mask)\n",
    "        print(\"bert_out after mean pooling: \", sentence_embeddings)\n",
    "\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        print(\"bert_out after normalization: \", sentence_embeddings)\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  {'input_ids': tensor([[    0,   100,   524,    10,  3645,    13,    61,    38,    74,   101,\n",
      "             7,   120,    63, 33183, 11303,     2]], device='cuda:0'), 'att_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "input_ids:  tensor([[    0,   100,   524,    10,  3645,    13,    61,    38,    74,   101,\n",
      "             7,   120,    63, 33183, 11303,     2]], device='cuda:0')\n",
      "bert_out:  BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4691,  0.3588, -0.3893,  ...,  0.9207,  0.3005, -0.6692],\n",
      "         [ 1.1010,  2.2223, -0.7313,  ...,  1.1773, -0.2689, -1.0141],\n",
      "         [ 1.3093,  2.2479, -0.0480,  ...,  1.0683,  0.2182,  0.1668],\n",
      "         ...,\n",
      "         [-0.5873, -0.0315,  0.2894,  ...,  1.2412,  0.6698, -0.5678],\n",
      "         [-0.7918, -0.3601, -0.9375,  ...,  1.2360,  0.7064, -0.8657],\n",
      "         [ 0.1682,  0.1122, -1.0189,  ...,  0.9203,  0.1400, -1.0907]]],\n",
      "       device='cuda:0'), pooler_output=tensor([[-0.1987, -0.1665,  0.0523,  ..., -0.4410, -0.5610,  0.0332]],\n",
      "       device='cuda:0'), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "bert_out after mean pooling:  tensor([[-0.0602,  0.3933, -0.8234,  ...,  1.2155,  0.2167, -1.0540]],\n",
      "       device='cuda:0')\n",
      "bert_out after normalization:  tensor([[-0.0024,  0.0156, -0.0327,  ...,  0.0482,  0.0086, -0.0418]],\n",
      "       device='cuda:0')\n",
      "All embeddings: tensor([[-0.0024,  0.0156, -0.0327,  ...,  0.0482,  0.0086, -0.0418]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "\n",
    "model = Sentence_Bert(pretrained_repo)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        print(\"batch: \", batch)\n",
    "\n",
    "        embeddings = model(input_ids = batch['input_ids'], att_mask = batch['att_mask'])\n",
    "        all_embeddings.append(embeddings)\n",
    "    \n",
    "all_embeddings = torch.cat(all_embeddings)\n",
    "\n",
    "print(\"All embeddings:\", all_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g_retriever",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
