{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.6 Subword Embedding\n",
    "\n",
    "In English, words such as \"helps\", \"helped\", and \"helping\" are inflected forms of the same word \"help\". The relationship between \"dog\" and \"dogs\" is the same as that between \"cat\" and \"cats\", and the relationship between \"boy\" and \"boyfriend\" is the same as that between \"girl\" and \"girlfriend\". In other languages such as French and Spanish, many verbs have over 40 inflected forms, while in Finnish, a noun may have up to 15 cases. In linguistics, morphology studies word formation and word relationships. However, the internal structure of words was neither explored in word2vec nor in GloVe.\n",
    "\n",
    "## 15.6.1 fastText Model\n",
    "\n",
    "Recall how words are represented in word2vec. In both the skip-gram model and the CBOW, different inflected forms of the word are directly represented by different vectors without shared parameters. To use morphological information, the fastText model proposed a subword embedding approach, where a subword is a character n-gram. Instead of learning word-level vector representations, fastText can be cconsidered as the subword-level skip-gram, where each center word is represented by the sum of its subword vectors.\n",
    "\n",
    "Let's illustrate how to obtain subwords for each center word in fastText using the word \"where\". First, add special characters \"<\" and \">\" at the beginning and end of the word to distinguish prefixes and suffixes from other subwords. Then, extract character n-grams from the word. For example, when $n=3$, we obtain all subwords of length 3: \"< wh\", \"whe\", \"ere\", \"re >\", and the special subword \"< where > \".\n",
    "\n",
    "In fastText, for any word $w$, denote be $\\mathcal{G}_w$ the union of all its subwords of length between 3 and 6 and its special subword. The vocab is the union of the subwords of all words. Letting $\\mathbf{z}_g$ be the vector of subword $g$ in the dictionary, the vector $\\mathbf{v}_w$ for word $w$ as a center word in the skip-gram model is the sum of its subword vectors:\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "\\mathbf{v}_w = \\sum_{g \\in \\mathcal{G}_w} \\mathbf{z}_g\n",
    "\n",
    "$$\n",
    "\n",
    "The rest of fastText is the same as the skip-gram model. Compared with the skip-gram model, the vocabulary in fastText is larger, resulting in more model parameters. Besides, to calculate the representation of a word, all its subword vectors have to be summed, leading to higher computational complexity.\n",
    "\n",
    "\n",
    "## 15.6.2 Byte Pair Encoding (BPE)\n",
    "\n",
    "In fastText, all the extract subwords have to be of the specified lengths, such as 3 to 6, thus the vocab size cannot be predefined. To allow for variable-length subwords in a fixed size vocabulary, we can apply a compression algorithm called byte pair encoding (BPE) to extract subwords.\n",
    "\n",
    "BPE performs a statistical analysis of the training dataset to discover common symbols within a word, such as consecutive characters of arbitrary length. Starting from symbols of length 1, BPE iteratively merges the most frequent pair of consecutive symobls to produce new longer symbols. Note that for efficiency, pairs crossing word boundaries are not considered. In the end, we can use such symbols as subwords to segement words. Byte pair encoding and its variants has been used for input representations in popular NLP pretrianing models such as GPT-2 and RoBERTa. In the following, we will illustrate how BPE works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "'_', '[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Key of `pairs` is a tuple of two consecutive symbols\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return max(pairs, key=pairs.get) # Key of `pairs` with the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_freqs:  {'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}\n",
      "token_freqs 0:  {'f a s t _': 4, 'f a s t e r _': 3, 'ta l l _': 5, 'ta l l e r _': 4}\n",
      "merge #1: ('t', 'a')\n",
      "token_freqs 1:  {'f a s t _': 4, 'f a s t e r _': 3, 'tal l _': 5, 'tal l e r _': 4}\n",
      "merge #2: ('ta', 'l')\n",
      "token_freqs 2:  {'f a s t _': 4, 'f a s t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "merge #3: ('tal', 'l')\n",
      "token_freqs 3:  {'fa s t _': 4, 'fa s t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "merge #4: ('f', 'a')\n",
      "token_freqs 4:  {'fas t _': 4, 'fas t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "merge #5: ('fa', 's')\n",
      "token_freqs 5:  {'fast _': 4, 'fast e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "merge #6: ('fas', 't')\n",
      "token_freqs 6:  {'fast _': 4, 'fast er _': 3, 'tall _': 5, 'tall er _': 4}\n",
      "merge #7: ('e', 'r')\n",
      "token_freqs 7:  {'fast _': 4, 'fast er_': 3, 'tall _': 5, 'tall er_': 4}\n",
      "merge #8: ('er', '_')\n",
      "token_freqs 8:  {'fast _': 4, 'fast er_': 3, 'tall_': 5, 'tall er_': 4}\n",
      "merge #9: ('tall', '_')\n",
      "token_freqs 9:  {'fast_': 4, 'fast er_': 3, 'tall_': 5, 'tall er_': 4}\n",
      "merge #10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "print(\"token_freqs: \", token_freqs)\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f\"token_freqs {i}: \", token_freqs)\n",
    "    print(f'merge #{i + 1}:', max_freq_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_', 'taller_', 'faster_', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast_', 'fast er_', 'tall_', 'tall er_']\n"
     ]
    }
   ],
   "source": [
    "print(list(token_freqs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        # Segment token with the longest possible subwords from symbols\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter_']\n",
    "print(segment_BPE(tokens, symbols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dive2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
